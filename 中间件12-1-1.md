





# 定时任务 CronJob



## Xxl-Job





# 消息队列 Kafka



在进行第三方渠道打卡数据同步时，在同步完成时，需要通过mq发送消息：

![image-20231123160805854](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311231608917.png)







其对应的核心代码为：

```java
/**
 * 推送打卡数据
 * 1.基于员工进行分组推送
 * 2.每个消息最多600条 ； 循环推送
 * 3.key 使用员工ID
 *
 * @param resList
 * @param entId
 */
public void pushMsg(List<ChannelCheckInDataRes> resList, Long entId) {
    if (CollectionUtils.isEmpty(resList)) {
        return;
    }

    // 每个消息的最大条数
    String countAbs = MokaConfig.getInstance().getStrPropertyFromDefault(ApolloConstants.HCM_MOBILE_MOKA_ABSENCE_MSG_COUNT);
    int partitionRecord = resList.size();
    if (StringUtils.isNotBlank(countAbs)) {
        partitionRecord = Integer.parseInt(countAbs);
    }

    // 通过员工号进行分组
    Map<Long, List<ChannelCheckInDataRes>> employeeDataMap = resList.stream().collect(Collectors.groupingBy(ChannelCheckInDataRes::getEmployeeId));
    for (Long employeeId : employeeDataMap.keySet()) {
        List<ChannelCheckInDataRes> employeeCheckInDataList = employeeDataMap.get(employeeId);
        List<List<ChannelCheckInDataRes>> checkInLists = ListUtils.partition(employeeCheckInDataList, partitionRecord);
        for (List<ChannelCheckInDataRes> singleEmployeeDataList : checkInLists) {
            AbsClockCheckInDataDto absClockCheckInDataDto = new AbsClockCheckInDataDto();
            absClockCheckInDataDto.setEntId(entId);
            absClockCheckInDataDto.setBuId(singleEmployeeDataList.get(0).getBuId());
            absClockCheckInDataDto.setEmployeeId(employeeId);
            absClockCheckInDataDto.setCheckInDataResDtoList(singleEmployeeDataList);

            Message message = Message.builder().sendTime(new Date())
                    .msgId(employeeId)
                    .isSequential(false)
                    .topic(getTopic(entId))
                    .body(JacksonUtils.toJson(absClockCheckInDataDto))
                    .build();
            log.info("HcmAbsenceMsgProducer pushMsg -{}", JacksonUtils.toJson(message));
            defaultNoLogMsgProducer.pushMsg(message);
        }
    }

}
```







## 使用 ContainerFactory 批量消费



打卡业务中，在调用第三方接口得到打卡数据后，会发送一个kafka消息，而该消息的消费者如下：

```java
@Slf4j
@Component
public class BatchSyncClockInRecordListener {


    @Resource
    private BatchSyncChannelRecordService batchSyncChannelRecordService;


    @KafkaListener(topics = {"${topic.hcm_abs_batch_sync_channel_record}"},
            groupId = "${group.hcm_abs_batch_sync_channel_record}",
            containerFactory = "batchConsumeListenContainerFactory")
    public void handleClockRecord(List<ConsumerRecord<String, String>> records, Acknowledgment ack) {

        ack.acknowledge();

        try {
            log.info("当前批次处理的消息: {}", records);
            Map<String, List<ChannelCheckInDataResDto>> employeeUniqkeyChannelRecordsMap = buildEmployeeUniqKeyChannelRecordsFromMsgs(records);

            batchSyncChannelRecordService.handleChannelRecords(employeeUniqkeyChannelRecordsMap);
        } catch (Exception e) {
            log.error("打卡记录同步消费失败 messages: {}", JacksonUtils.toJson(records), e);
        }
    }
  
  
  /*
  *
  * 从 Kafka 消息中提取批量同步的打卡记录数据，按照员工的唯一键进行分组，
  * 构建一个映射关系，其中键是员工的唯一键，值是该员工对应的打卡记录列表
  */

    private Map<String, List<ChannelCheckInDataResDto>> buildEmployeeUniqKeyChannelRecordsFromMsgs(List<ConsumerRecord<String, String>> records) {
        if (CollectionUtils.isEmpty(records)) {
            log.warn("没有消费的消息");
            return Maps.newHashMap();
        }
        Map<String, List<ChannelCheckInDataResDto>> employeeUniqKeyChannelRecordsMap = Maps.newHashMap();
        for (ConsumerRecord<String, String> record : records) {
            try {
                Message message = JacksonUtils.fromJson(record.value(), Message.class);
                if (Objects.isNull(message)) {
                    log.warn("BatchSyncClockInRecordListener 消费失败，消息为空");
                    continue;
                }
                BatchSyncClockInRecordResDto batchChannelRecordDto = JacksonUtils.fromJson(message.getBody(), BatchSyncClockInRecordResDto.class);
                if (Objects.isNull(batchChannelRecordDto)) {
                    log.warn("BatchSyncClockInRecordListener 消费失败，消息体为空");
                    continue;
                }
                Tenant tenant = Tenant.of(batchChannelRecordDto.getEntId(), batchChannelRecordDto.getBuId());
                String employeeUniqKey = ClockRecordUtil.generateEmployeeUniqKey(tenant, batchChannelRecordDto.getEmployeeId());
                List<ChannelCheckInDataResDto> checkInDataResDtos = Optional.ofNullable(batchChannelRecordDto.getCheckInDataResDtoList()).orElse(Lists.newArrayList());
                checkInDataResDtos = checkInDataResDtos.stream().filter(channelRecord -> Objects.nonNull(channelRecord.getCheckInTime())).collect(Collectors.toList());
                List<ChannelCheckInDataResDto> list = employeeUniqKeyChannelRecordsMap.get(employeeUniqKey);
                if (CollectionUtils.isNotEmpty(list)){
                    list.addAll(checkInDataResDtos);
                }else {
                    employeeUniqKeyChannelRecordsMap.put(employeeUniqKey, checkInDataResDtos);
                }
            } catch (Exception e) {
                log.error("当前消息解析失败 record: {}", record);
            }
        }
        log.info("当前处理的员工打卡信息: {}", JacksonUtils.toJson(employeeUniqKeyChannelRecordsMap));
        return employeeUniqKeyChannelRecordsMap;
    }

}

```



可以看出来，`@KafkaListener`指定了`containerFactory`，即自定义配置了消息监听容器工厂`bean`:

```java
package com.moka.hcm.absence.clock.common.kafka.consume.factory;

@Configuration
@EnableKafka
public class BatchConsumeConfigContainerFactory {

    private static final Logger log = LoggerFactory.getLogger(KafkaConsumerConfig.class);
    @Value("${kafka.broker.hosts}")
    private String brokers;
    @Value("${kafka.zk.hosts}")
    private String zkHosts;
    @Value("${kafka.consumer.enable.auto.commit}")
    private boolean enableAutoCommit;
    @Value("${kafka.consumer.session.timeout}")
    private String sessionTimeout;
    @Value("${kafka.consumer.auto.commit.interval}")
    private String autoCommitInterval;
    @Value("${kafka.consumer.group.id}")
    private String groupId;
    @Value("${kafka.consumer.auto.offset.reset}")
    private String autoOffsetReset;
    @Value("${kafka.consumer.concurrency}")
    private int concurrency;
    @Value("${kafka.consumer.max.poll.records:5}")
    private int maxPollRecords;
    @Value("${kafka.consumer.max.poll.interval:300000}")
    private int maxPollInterval;

    public BatchConsumeConfigContainerFactory() {
    }

    @Bean
    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> batchConsumeListenContainerFactory() {
        log.info("---KafkaListenerContainerFactory---");
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        log.info("---ConcurrentKafkaListenerContainerFactory---new");
        factory.getContainerProperties().setAckMode(AbstractMessageListenerContainer.AckMode.MANUAL);
        factory.setConsumerFactory(this.consumerFactory());
        log.info("---ConcurrentKafkaListenerContainerFactory---setConsumerFactory");
        factory.setConcurrency(this.concurrency); //消费者并发消费线程数为6
        factory.getContainerProperties().setPollTimeout(2500L);
        factory.getContainerProperties().setIdleEventInterval(6000L);
        // 开启批量消费
        factory.setBatchListener(true);
        log.info("---ConcurrentKafkaListenerContainerFactory---return--");
        return factory;
    }

    public ConsumerFactory<String, String> consumerFactory() {
        log.info("---ConsumerFactory---");
        return new DefaultKafkaConsumerFactory<>(this.consumerConfigs());
    }

    public Map<String, Object> consumerConfigs() {
        log.info("---consumerConfigs---");
        Map<String, Object> propsMap = new HashMap<>();
        propsMap.put("bootstrap.servers", this.brokers);
        propsMap.put("max.poll.records", this.maxPollRecords);
        propsMap.put("enable.auto.commit", this.enableAutoCommit);
        propsMap.put("auto.commit.interval.ms", this.autoCommitInterval);
        propsMap.put("session.timeout.ms", this.sessionTimeout);
        propsMap.put("max.poll.interval.ms", this.maxPollInterval);
        propsMap.put("key.deserializer", LongDeserializer.class);
        propsMap.put("value.deserializer", StringDeserializer.class);
        propsMap.put("group.id", "pre-hcm_mobile_push");
        propsMap.put("auto.offset.reset", this.autoOffsetReset);
        return propsMap;
    }

}

```



对应的kafka配置为：

```yaml
#-------------------------------kafka 配置----------------------------------------------------------------
kafka:
  # 自定义分区
  partitioner.class: com.moka.hcm.kafka.paritioner.PushMsgPartitioner
  # zkHost
  zk.hosts:
  # topic
  ############ consumer ##########
  # 自动提交offset
  consumer.enable.auto.commit: false
  # 客户端会话超时时间
  consumer.session.timeout: 60000
  # kafka批量提交大小
  consumer.auto.commit.interval: 10
  # 无提交offset时从最新的位置开始消费
  consumer.auto.offset.reset: latest
  # 消费者分组
  consumer.group.id: hcm_mobile
  # 消费者并发消费线程数
  consumer.concurrency: 6
  ############ producer #######
  producer.retries: 3
  # 消息批量发送大小
  producer.batch.size: 4096000
  # 在消息批量发送前等待其他其他消息加入的时间
  producer.linger: 1
  # 消息发送内存buffer大小
  producer.buffer.memory: 4096000
  # 一次性拉取的数量待定  ${kafka.consumer.max.poll.records:5}
  max.poll.records: 100
```







# WebSocket



假设有这样一个场景：服务端的资源经常在更新，客户端需要尽量及时地了解到这些更新发生后展示给用户，如果是 HTTP 1.1，通常会开启 ajax 请求询问服务端是否有更新，通过定时器反复轮询服务端响应的资源是否有更新。

[![ajax 轮询](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311241752280.png)](https://img-blog.csdnimg.cn/20200627220503451.png)

在长时间不更新的情况下，反复地去询问会对服务器造成很大的压力，对网络也有很大的消耗，如果定时的时间比较大，服务端有更新的话，客户端可能需要等待定时器达到以后才能获知，这个信息也不能很及时地获取到。

而有了 WebSocket 协议，就能很好地解决这些问题，WebSocket 可以反向通知的，通常向服务端订阅一类消息，服务端发现这类消息有更新就会不停地通知客户端。

[![WebSocket](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311241752463.png)](https://img-blog.csdnimg.cn/20200627220503449.png)



> **WebSocket** 是基于 TCP 的一种新的应用层网络协议。它实现了浏览器与服务器全双工通信，即允许服务器主动发送信息给客户端。因此，在 **WebSocket** 中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输，客户端和服务器之间的数据交换变得更加简单。

🍊WebSocket是一种网络通信协议，它提供了在单个TCP连接上进行全双工通信的能力。这意味着在一个连接上，服务器和客户端都可以同时发送和接收数据。

🍊WebSocket与HTTP协议不同，它使用了独立的端口，并且在建立连接后，不需要在每次数据传输时重新建立连接。这使得它特别适合于实时应用程序，例如聊天，在线游戏和股票交易等，这些应用程序需要高速，双向的数据传输。

🍊WebSocket协议是HTML5标准的一部分，因此它可以在现代浏览器中使用。WebSocket API可以在JavaScript中使用，这样就可以在网页上直接使用WebSocket进行通信。

🍊WebSocket使用了一种称为握手的连接建立机制，服务器和客户端在建立连接之前需要进行握手。握手是通过HTTP协议完成的，但是一旦建立连接后，数据传输将使用WebSocket协议。



WebSocket通信的流程如下：

- 客户端发送一个HTTP请求，请求的目的是为了要建立一个WebSocket连接。

- 服务器收到请求后，给出一个HTTP响应，并升级连接到WebSocket协议。
- 客户端和服务器之间建立一个WebSocket连接。
- 客户端和服务器之间可以进行双向通信，发送文本和二进制数据。
- 当客户端或服务器关闭连接时，WebSocket连接也会关闭。
- 与 HTTP 通信不同的是，WebSocket 通信是基于TCP的，所以它是一个持久连接。它允许服务器主动发送信息给客户端，而不是等待客户端的请求。这使得 WebSocket 通信成为了实现实时应用的理想选择。
- 如下图所示，发起一个websocket链接之后，请求和响应的参数里会有一些websocket相关的参数，该报文中有一个upgrade首部，它的作用是告诉服务端需要将通信协议切换到websocket![image.png](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311241758205.png)









## 相对于http的区别



**WebSocket协议和HTTP协议有以下几点区别：**

通信方式不同: HTTP协议是一种请求-响应协议，客户端发送请求给服务器，服务器返回响应。而WebSocket协议是一种全双工协议，客户端和服务器都可以主动发送消息。

链接状态不同: HTTP协议是无状态的，每次请求都是独立的。而WebSocket协议是有状态的，建立了连接之后，客户端和服务器之间可以维持长连接。

数据传输不同: HTTP协议是基于文本的，而WebSocket协议是基于二进制的。

延迟不同: HTTP协议每次请求都需要建立连接，等待响应，传输数据，释放连接，这整个过程都需要一些时间。而WebSocket协议只需要建立一次连接，之后就可以高效地进行数据传输，所以延迟更小。









# Redisson





## Guava 单机限流 RateLimiter



Guava的限流基于**令牌桶**算法实现，提供了平滑突发限流方案和平滑预热限流两种方式。

平滑突发限流具备以下特点：

1. 以固定的速率生成令牌

2. RateLimiter使用令牌桶算法，会进行令牌的累积，如果获取令牌的频率比较低，则不会导致等待，直接获取令牌。

3. RateLimiter在没有足够令牌发放时，采用滞后处理的方式，也就是前一个请求获取令牌所需等待的时间由下一次请求来承受，也就是代替前一个请求进行等待。

平滑预热限流，是在平滑突发限流的基础上，增加了带有预热期的一种平滑限流，即它启动后会有一段预热期，逐步将分发频率提升到配置的速率。平滑预热限流，通过动态调整令牌发放速度，可以让流量变化更加平滑。假设，现在有一个接口，限定100QPS，当前如果没有请求到来，令牌桶就会有100个令牌，如果突发的到来100个请求，这个时候就会瞬间消耗掉令牌，并产生较大的冲击力。而平滑预热限流，则可以根据桶内的令牌数量动态控制令牌的发放速率，让忙时流量和闲时流量可以互相平滑过渡。








## 分布式限流 RRateLimiter



我们在对接钉钉、微信等生态服务（同步第三方渠道的打卡数据）时，需要调用它们在开放平台的API，然而无论是lark、还是钉钉、企业微信，都限制了外部调用api的频率（限流要求），因为一旦调用钉钉API频率超限，会触发至少5分钟的限流熔断，这5分钟内任何API调用都会报错。所以我们作为服务调用方，更要将调用服务的请求限流。例如钉钉官方文档给出了如下解释：

[如何处理钉钉服务端API限流](https://open.dingtalk.com/document/orgapp/how-to-process-api-throttling-on-the-dingtalk-server)

**解决办法一般有以下几种方式:**

- **通用**
  - 因为qps限流限制时间是1秒，所以当遇到限流错误时，可以在程序中 sleep 1 秒，然后继续执行
  - **队列调用**：针对单个应用API的QPS限流，可通过队列调用的方式解决
- **主动单机限流**：这个是根本上解决限流的方法，如果是单个服务器调用钉钉的服务端api，可以使用类似**Guava RateLimiter**的限流sdk，来自由控制调用频率。
- **主动分布式限流**：如果是多个服务器调用钉钉的服务端api，可以使用分布式缓存，其中缓存的key是当前的秒级时间戳，value就是调用次数。



同步第三方渠道打卡数据时，所涉及到的限流器：

![image-20231123204008742](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311232040895.png)

该**`larkAbsenceAcquireByEnt` 方法主要做了以下几件事：**

- 根据企业（`entId`）进行限流。
- 先通过 `initRateLimiter` 方法初始化或获取与指定键相关的限流器。
- 通过 `tryAcquire` 方法尝试获取令牌，以判断是否允许执行当前请求。
- `LIMIT_BUFFER` 是一个缓冲值，用于在设置限流速率时考虑一些余量。
- `LIMITER_WAIT_TIME` 是获取令牌的最大等待时间，单位是秒。
- 如果无法获取令牌，则抛出自定义异常，并记录限流信息。









# Sharding—JDBC



## 一、分库分表背景

传统的将数据集中存储至单⼀数据节点的解决方案，在性能、可用性和运维成本这三方面已经难于满足互联网的海量数据场景。

随着业务数据量的增加，原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。

当业务系统的数据容量接近或超过单台服务器的容量、QPS/TPS接近或超过单个数据库实例的处理极限等，此时，往往是采用垂直和水平结合的数据拆分方法，把数据服务和数据存储分布到多台数据库服务器上。

> 逻辑库与物理库的区别 :astonished:

## 二、分库分表组件比对

| 组件          | 使用方式                                                     | 优点                                                         | 缺点                                                         |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| mycat         | 独立业务服务部署,使用方雨mycat模拟的虚拟库、虚拟表交互（反向代理） | 1、分库分表的逻辑对外层隐藏，使用方不用关注内部实现逻辑，可以按照单库、单表的方式使用，接入简单2、中间层更便于实现监控、数据迁移、连接管理等功能 | 需要一个独立的部署，且有单点故障风险需要注意，运维成本高，且单库分表不支持join分库分表策略配置比较落后、且繁琐社区不太活跃 |
| sharding-jdbc | 客户端直连，仅仅是一个jdbc的增强实现，使用方式和普通的jdbc相似，配置数据访问对应的表结构（正向代理） | 1、客户端直连，性能更好2、分库分表策略丰富，接入后使用简单3、自定义策略方便4、该组件不需要单独的运维成本5、社区活跃度高 | 使用需要对该组件有一定的学习成本，需要使用方来控制分库分表接入 |





## 三、sharding-jdbc实战

### 1、环境准备

以假勤项目数据量大且常用的数据表日报记录为例

两个数据库：hcm-pp,hcm-pp1，

假勤日报记录表：hcm_pp_daily_report，两个库中都有hcm_pp_daily_report_0,hcm_pp_daily_report_1,hcm_pp_daily_report_2,hcm_pp_daily_report_3,hcm_pp_daily_report_4,hcm_pp_daily_report_5，共12张表

分库通过：daily_report_date字段，日报数据是2023年之前的数据放入hcm-pp库中的表，数据是2023年之后的放入hcm-pp1库中

分表通过：employee_id进行分表

![image2023-11-30_20-23-3](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011632664.png)

项目中引入依赖：

![image2023-11-30_20-24-8](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011633479.png)



配置文件：

![image2023-11-30_20-29-4](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011633595.png)



分库算法实现：

![image2023-11-30_20-29-41](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011634002.png)







分表算法实现：

![image2023-11-30_20-30-5](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011634620.png)









### 2、分表方法分析与选取



| 方法名称 | 说明                                                         | 注意点                                       |
| -------- | ------------------------------------------------------------ | -------------------------------------------- |
| 时间     | 适合有归档需求，查询的场景有明显的时间属性，且查询场景都是时间热点数据 | 如果业务不稳定，可能会造成比较严重的数据倾斜 |
| 范围     | 使用数字类型的字段进行分表，一般自增型，主键ID               | 注意每个表的初始化id以及自增服务，后期扩容   |
| 取模     | 一般使用一些与业务相关的id，比如用户id，对我们的表数量进行取模，然后确认路由到表。使用得比较多方法 | 前期需要规划好数量和扩容方案以及迁移方案     |

### 3、分片策略

**标准分片策略（StandardShardingStrategy）**： 只支持对单个分片键为依据的分库分表，并提供了两种分片算法：

- **PreciseShardingAlgorithm（精准分片）**：在使用标准分片策略时，精准分片算法时必须实现的算法，用于SQL含有 = 和 IN 的分片处理；返回目标的库名称或者表名称为单个名称

  ![image2023-12-1_14-38-51](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011636406.png)



- **RangeShardingAlgorithm（范围分片）**：非必选的，用于处理含有 BETWEEN AND 的分片处理。，如下图，返回的目标库或者表名称是一个集合

  ![image2023-12-1_14-36-1](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011636781.png)





注意 :exclamation::exclamation: 一旦我们没配置范围分片算法，而SQL中又用到 BETWEEN AND 或者 LIKE 等，那么 SQL 将按全库、表路由的方式逐一执行，查询性能会很差。



**复合分片策略**

(1)使用场景
使用场景：SQL 语句中有>，>=, <=，<，=，IN 和 BETWEEN AND 等操作符，不同的是复合分片策略支持对多个分片健操作。
标准分片策略，我们只是根据一个字段来查询，也就是一个分片键。下边我们同时实现以id和type为条件来实现查询

(2)配置
我们修改一下原配置，`standard.sharding-column` 切换成 `complex.sharding-columns` 复数，分片健上再加一个 user_id ，分片策略名变更为 complex ，`complex.algorithm-class-name` 替换成我们自定义的复合分片算法。

```yaml
#spring.shardingsphere.sharding.tables.course.table-strategy.complex.sharding-columns=sharding-column=employee_id,daily_report_date

```

当我们实现分片策略是，传入的参数就是各字段集合

![image2023-12-1_14-53-51](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011639133.png)





(3)复合分片情况下，涉及跨库事务该如何处理？
