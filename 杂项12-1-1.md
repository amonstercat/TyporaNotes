# 发布流程及规范



## 1、基本规范

- - 基础分支
    - master: 主开发分支，开发新代码从**release**分支checkout开发分支（下面的图有误），开发测试完成合并回master分支，master分支可以保证基本稳定，但可能会有小bug
    - release: 线上分支，基本上跟线上运行的代码版本一致（只有代码进release等待上线的短暂时间不一致），完全稳定的分支
    - integration: 集成测试分支，每个sprint上线前会从master checkout integration分支进行集成测试,
    - gray-release: 灰度分支，每次集成测试通过后会从integration checkout gray-release分支进行灰度上线
  - 其中针对会进行灰度的代码服务，四个分支都会有。其他的代码服务如果迭代非常少，可以只有master和release分支，所有线上代码仅从release分支拉取进行发布
  - 在以上四个基础分支的操作上，只允许有merge操作，不允许cherry-pick和直接push（特殊情况需要部门leader审批后操作）
  - 哪个分支发现bug就在哪个分支checkout修复分支修复并测试，修复完成合并回该分支

## 2、研测迭代流程/周期/环境/分支

| 迭代周期 |           开发           |         集成测试          | canary（部分租户线上环境） |    全量线上    |
| :------: | :----------------------: | :-----------------------: | :------------------------: | :------------: |
| 代码分支 |          master          |        integration        |        gray-release        |    release     |
| 运行环境 | 开发测试环境（staging*） | 集成测试环境（Staging-2） |        灰度线上环境        |      线上      |
| 时间节点 |                          |      第一周周二晚上       |       第一周周四晚上       | 第二周周四晚上 |



![1](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311231148339.png)



各阶段具体事宜：

一、宣讲

产品：宣讲需求。默认是周三进行（产研群里会有通知）

开发&QA：宣讲之前可以根据产研大群发的资料进行自己的理解，在宣讲时明确需求，提自己的建议及疑问。

二、估时

开发：

1、要对需求进行全方位的理解（注：考虑一定要周全，各种细节一定要了解彻底，否则需求不对返工代价很大）。

2、功能分析，可以产出一个脑图。

3、反讲

4、撰写技术方案

5、技术方案评审（根据需求的复杂程度和相关负责人对一下）

6、整理出涉及的人员、模块、影响点及接口。

7、估时

a、进入开发的时间（具体到天）

b、接口的定义时间

c、总体联调时间

d、跑case时间（自测，撰写testplan,联调，跑测试用例）

e、提测时间





三、开发

开发：明确代码写在哪个项目，形成代码，测试用例评审，自测，写testplan，联调，跑case，代码review（找到相关的leader来Review）。

QA：撰写测试用例，测试用例评审(给够开发跑case的时间，宣讲中的所有人员一起)。

四、提测

开发：

a、发邮件提测。

b、配合测试同学进行环境部署。

c、撰写上pre方案和上prod方案：包括环境的部署，各项工作跟进负责人，是否和其他开发撞模块等。

d、生成pr

五、测试

开发：配合测试同学，改bug。

六、pre

开发：部署环境，配合测试同学，改bug

七、prod（默认周四）

开发：部署环境，配合测试同学





迭代流程的宏观图（见下图）：

![image2021-7-9_14-50-26](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311231148488.png)





hotfix流程如下：

![image2021-7-9_16-32-1](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311231149612.png)







## 3、上线分支管理



<img src="https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311231149660.png" alt="image-20231107160336225" style="zoom:67%;" />



---





![image2022-8-18_14-35-31](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311231149448.png)



### 3.1 Sprint发版

**根据业务规模不同，对稳定性和效率要求不同，以下某些阶段可以不强求，但如存在的阶段必须按照对应的规范进行操作（CI/CD系统也仅支持规范以内操作）**

需求开发： 从**release**分支check out出个人分支进行开发，完成自测后合并回master，如有冲突需解决后测试；任何时候/gray-release/release/分支如果有代码变更，均会自动合并至master。

集成测试： 从master分支check out(或merge)出integration分支，并发布至集成测试环境进行测试，此期间release分支和gray-release代码有变更会自动合并至integration，如有冲突需解决后测试。

灰度上线： 从integration分支（完成集成测试的）merge至gray-release分支，并发布至灰度线上环境，进行灰度验证；此期间release分支代码有变更会自动合并至gray-release分支，如有冲突需解决冲突后完成测试。

全量上线： 从Gray-release分支merge至release分支，并发布至线上环境，

### 3.2 Hotfix

全量环境的hotfix：从release check out hotfix分支；bug修复完成后进行合并到release分支进行上线； 合并到release分支后会自动分别合并到 gray-release/integration/master

灰度环境的hotfix：从gray-release check out hotfix 分支；bug修复完成后进行合并到gray-release/分支进行上线； 目前integration分支变更没有自动合并至master；是由integration->gray-release 时自动将integration 的单独变更合并会master





##  4、灰度发布 :astonished:





### People灰度方案



**前言**：

>  在平时的业务开发过程中，后端服务与服务之间的调用往往通过fegin，但是我们在调用服务的时候往往只需要写服务名就可以做到路由到具体的服务，这其中的原理相比大家都知道是SpringCloud的ribbon组件帮我们做了负载均衡的功能。灰度发布的核心就是路由，**如果我们能够重写ribbon默认的负载均衡算法是不是就意味着我们能够控制服务的转发呢**？



#### **调用链分析**

- 外部调用

  ![image2021-8-23_17-7-29](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281650090.png)

  ​      

  hcm-gateway 是在转发请求的时候，也会根据`Ribbon`从服务实例列表中选择一个对应的服务,然后选择转发.

  

- 内部调用

  ![image2021-8-23_17-7-49](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281651576.png)

   服务之间调用也通过`Ribbon`

   所以：我们通过重写ribbon默认的负载均衡算法就能够控制服务的转发



#### **方案详细设计**

 ==Eureka元数据==：

 Eureka的元数据有两种，分别为标准元数据和自定义元数据。    

| 元数据种类   | 解释                                                         |
| :----------- | :----------------------------------------------------------- |
| 标准元数据   | 主机名、IP地址、端口号、状态页和健康检查等信息，这些信息都会被发布在服务注册表中，用于服务之间的调用 |
| 自定义元数据 | 自定义元数据可以使用`eureka.instance.metadata-map`配置，这些元数据可以在远程客户端中访问，但是一般不会改变客户端的行为，除非客户端知道该元数据的含义 |



==更改自定义元数据==：

配置文件方式：（version为自定义元数据）

```
eureka.instance.metadata-map.version = gray-release
```

​     接口请求：

```
PUT /eureka/apps/{appID}/{instanceID}/metadata?key=value
```



==外部调用流程==：

![image2021-8-18_15-39-50](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281655437.png)





1. 用户请求首先到达Nginx然后转发到网关，此时gateway拦截器会根据用户携带请求`token`解析出对应的租户Id
2. 网关从Apollo配置中心拉取灰度用户列表，然后根据灰度用户策略判断该用户是否是灰度用户。如是，则给该请求添加**请求头**及**线程变量**添加信息`version=xxx`；若不是，则不做任何处理放行
3. 在gateway拦截器执行完毕后，gateway在进行转发请求时会通过负载均衡器Ribbon。
4. 负载均衡Ribbon被重写。当请求到达时候，Ribbon会取出存入**线程变量**值`version`。于此同时，Ribbon还会取出所有缓存的服务列表（定期从eureka刷新获取最新列表）及其该服务的`metadata-map`信息。然后取出服务`metadata-map`的`version`信息与线程变量`version`进行判断对比，若值一致则选择该服务作为返回。若所有服务列表的version信息与之不匹配，则返回任意一个服务，此时Ribbon选取不到对应的服务则会报错！
5. 当服务为非灰度服务，即没有version信息时，此时Ribbon会收集所有非灰度服务列表，然后利用Ribbon默认的规则从这些非灰度服务列表中返回一个服务。
6. gateway通过Ribbon将请求转发到后端服务后，可能还会通过`fegin`或`resttemplate`调用其他服务，但是无论是通过`fegin`还是`resttemplate`，他们最后在选取服务转发的时候都会通过`Ribbon`。
7. 那么在通过`fegin`或`resttemplate`调用另外一个服务的时候需要设置一个拦截器，将请求头`version=xxx`给带上，然后存入线程变量。
8. 在经过`fegin`或`resttemplate` 的拦截器后最后会到Ribbon，Ribbon会从**线程变量**里面取出`version`信息。然后重复步骤（4）和（5）

外部调用流程如下：

<img src="https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281657012.png" alt="image2021-8-23_21-14-26" style="zoom:50%;" />





==内部调用流程：==



<img src="https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281658623.png" alt="image2021-8-23_21-30-40" style="zoom:50%;" />



1. 发起内部调用请求，请求会通过负载均衡器Ribbon
2. 当前服务获取自己`metadata-map`配置，判断是否存在version
3. 如果当前服务的`metadata-map存在version信息`，那么取出所有缓存的服务列表（定期从eureka刷新获取最新列表），然后取出服务`metadata-map`的`version`信息与当前服务`version`进行判断对比，若值一致则选择该服务作为返回。若所有服务列表的version信息与之不匹配，则返回任意一个服务，此时Ribbon选取不到对应的服务则会报错！
4. 如果当前服务的`metadata-map不存在version信息，直接返回服务列表中匹配的服务`





#### **方案实现**



**1、GateWay改造**

- 自定义拦截器`GrayFilter，从请求头中获取当前请求租户ID，从apollo中获取灰度租户列表，判断该租户是否为灰度租户,使用`HystrixRequestVariableDefault 设置线程变量
- 自定义GrayMetadataRule，重写ribbon默认的负载均衡算法

```java
@Slf4j
@Component
public class GrayMetadataRule extends ZoneAvoidanceRule {
 
 
    private Random random = new Random();
 
    @Override
    public Server choose(Object key) {
       log.info("GrayMetadataRule key-{}", key);
        String isGray = GrayHolder.isGray();
        List<Server> serverList = this.getPredicate().getEligibleServers(this.getLoadBalancer().getAllServers(), key);
        List<Server> backList = serverList.stream().filter(server -> {
            Map<String, String> metadata = ((DiscoveryEnabledServer) server).getInstanceInfo().getMetadata();
            String metaVersion = metadata.get(GrayHolder.GRAY_SERVICE_KEY);
            return isGray.equals(metaVersion);
        }).collect(Collectors.toList());
        if (CollectionUtils.isEmpty(backList)) {
            log.info("没有灰度服务，正常返回-{}", JSON.toJSONString(serverList));
            return serverList.get(random.nextInt(serverList.size()));
        } else {
            Server server = backList.get(random.nextInt(backList.size()));
            log.info("返回灰度服务-{}", JSON.toJSONString(server));
            return server;
        }
 
 
    }
     
    @Bean
    public IRule ribbonRule() {
        return new GrayMetadataRule();
    }
 
}
```





**2、后端服务改造**

1. 确认所有服务间的调用 是通过服务发现的形式；禁止调用方式为IP+端口
2. 引入灰度包（孙遥提供）

```xml
<dependency>
    <groupId>com.moka.gateway.sdk</groupId>
    <artifactId>gateway-sdk</artifactId>
    <version>4.0.2-RELEASE</version>
    <exclusions>
        <exclusion>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
        </exclusion>
    </exclusions>
</dependency>
```



**3、Kafka消息灰度设计**

方案一：灰度环境使用一套新的kafka环境，所有服务都部署一套

   <img src="https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281707242.png" alt="image2021-9-2_14-54-20" style="zoom:50%;" />


方案二：

![image2021-9-2_16-5-15](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281708863.png)

1. 需要建立对应的灰度 topic
2. 重写producer拦截器，判断当前服务是否是灰度，如果是灰度服务，发送到灰度 topic
3. 灰度消费者服务启动时javaagent修改所有监听的Topic; 加上前缀gray







**4、定时任务灰度**



方案1:

![image2021-9-2_22-19-8](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281711695.png)

1. 在xxljob管理后台新增 灰度执行器
2. 创建需要灰度执行器执行的任务
3. 灰度执行器读取灰度租户列表，只处理灰度租户的任务
4. 线上执行器读取灰度租户列表，过滤掉灰度租户



方案2:

![image2021-9-3_11-30-15](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281712306.png)



1. 调度器遍历执行器列表
2. 如果存在灰度服务，那么调度该机器执行，如果还存在其他的非灰度服务，继续调度执行
3. 如果不存在灰度服务，那么根据调度策略就行调度
4. 执行器执行逻辑同方案1的3、4

| 方案  | 优点                               | 缺点                             |
| :---- | :--------------------------------- | :------------------------------- |
| 方案1 | 不用改xxljob调度器的代码           | 需要单独配置灰度调度器及调度任务 |
| 方案2 | 不需要单独配置灰度调度器及调度任务 | 需要改xxljob调度器的代码         |



方案3【采用】：

引入定时任务统一接入包

```xml
<dependency>
    <groupId>com.starter.hcm</groupId>
    <artifactId>hcm-starter-job</artifactId>
    <version>xxx</version>
</dependency>
```

<img src="https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281715989.png" alt="image2022-3-29_10-31-24" style="zoom:33%;" />







#### 方案验证

调用链路设计如下：

![image2021-8-24_15-32-47](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281717809.png)



验证点：

1. Gateway的灰度路由策略是否达到预期
2. 服务之间的调用，灰度路由策略是否达到预期

这里以服务对接中的字段映射为例

1. 用户发起请求，请求到达gateway
2. 判断是否为灰度用户，如果是请求进入灰度链路，请求到达服务1，3
3. 如果为非灰度用户，请求到达2，4

![image2021-8-24_15-28-47](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281718402.png)



**Apollo配置：**配置租户1为灰度租户

![image2021-8-26_15-46-33](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281721859.png)




```xml
<application>
    <name>HCM-MOBILE</name>
    <instance>
        <instanceId>10.88.49.199:hcm-mobile:8786</instanceId>
        <hostName>10.88.49.199</hostName>
        <app>HCM-MOBILE</app>
        <ipAddr>10.88.49.199</ipAddr>
        <status>UP</status>
        <overriddenstatus>UNKNOWN</overriddenstatus>
        <port enabled="true">8786</port>
        <securePort enabled="false">443</securePort>
        <countryId>1</countryId>
        <dataCenterInfo class="com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo">
            <name>MyOwn</name>
        </dataCenterInfo>
        <leaseInfo>
            <renewalIntervalInSecs>10</renewalIntervalInSecs>
            <durationInSecs>90</durationInSecs>
            <registrationTimestamp>1629963368162</registrationTimestamp>
            <lastRenewalTimestamp>1629963719182</lastRenewalTimestamp>
            <evictionTimestamp>0</evictionTimestamp>
            <serviceUpTimestamp>1629963368047</serviceUpTimestamp>
        </leaseInfo>
        <metadata>
            <grayReleased>1</grayReleased>
            <management.port>8786</management.port>
        </metadata>
        <homePageUrl>http://10.88.49.199:8786/</homePageUrl>
        <statusPageUrl>http://10.88.49.199:8786/actuator/info</statusPageUrl>
        <healthCheckUrl>http://10.88.49.199:8786/actuator/health</healthCheckUrl>
        <vipAddress>hcm-mobile</vipAddress>
        <secureVipAddress>hcm-mobile</secureVipAddress>
        <isCoordinatingDiscoveryServer>false</isCoordinatingDiscoveryServer>
        <lastUpdatedTimestamp>1629963368162</lastUpdatedTimestamp>
        <lastDirtyTimestamp>1629963368036</lastDirtyTimestamp>
        <actionType>ADDED</actionType>
    </instance>
    <instance>
        <instanceId>hcm-mobile-585b45c8fb-jqvjv:8080</instanceId>
        <hostName>hcm-mobile.staging-12.svc.k8s.staging.mokahr.com</hostName>
        <app>HCM-MOBILE</app>
        <ipAddr>hcm-mobile.staging-12.svc.k8s.staging.mokahr.com</ipAddr>
        <status>UP</status>
        <overriddenstatus>UNKNOWN</overriddenstatus>
        <port enabled="true">8080</port>
        <securePort enabled="false">443</securePort>
        <countryId>1</countryId>
        <dataCenterInfo class="com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo">
            <name>MyOwn</name>
        </dataCenterInfo>
        <leaseInfo>
            <renewalIntervalInSecs>10</renewalIntervalInSecs>
            <durationInSecs>90</durationInSecs>
            <registrationTimestamp>1629908866887</registrationTimestamp>
            <lastRenewalTimestamp>1629963721297</lastRenewalTimestamp>
            <evictionTimestamp>0</evictionTimestamp>
            <serviceUpTimestamp>1629908866681</serviceUpTimestamp>
        </leaseInfo>
        <metadata>
            <management.port>8080</management.port>
        </metadata>
        <homePageUrl>http://hcm-mobile.staging-12.svc.k8s.staging.mokahr.com:8080/</homePageUrl>
        <statusPageUrl>http://hcm-mobile.staging-12.svc.k8s.staging.mokahr.com:8080/actuator/info</statusPageUrl>
        <healthCheckUrl>http://hcm-mobile.staging-12.svc.k8s.staging.mokahr.com:8080/actuator/health</healthCheckUrl>
        <vipAddress>hcm-mobile</vipAddress>
        <secureVipAddress>hcm-mobile</secureVipAddress>
        <isCoordinatingDiscoveryServer>false</isCoordinatingDiscoveryServer>
        <lastUpdatedTimestamp>1629908866887</lastUpdatedTimestamp>
        <lastDirtyTimestamp>1629908866601</lastDirtyTimestamp>
        <actionType>ADDED</actionType>
    </instance>
</application>




-----------------------------------





<application>
    <name>HCM-CORE-BASE</name>
    <instance>
        <instanceId>hcm-core-base-5d5987677d-l724k:8080</instanceId>
        <hostName>hcm-core-base.staging-12.svc.k8s.staging.mokahr.com</hostName>
        <app>HCM-CORE-BASE</app>
        <ipAddr>hcm-core-base.staging-12.svc.k8s.staging.mokahr.com</ipAddr>
        <status>UP</status>
        <overriddenstatus>UNKNOWN</overriddenstatus>
        <port enabled="true">8080</port>
        <securePort enabled="false">443</securePort>
        <countryId>1</countryId>
        <dataCenterInfo class="com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo">
            <name>MyOwn</name>
        </dataCenterInfo>
        <leaseInfo>
            <renewalIntervalInSecs>10</renewalIntervalInSecs>
            <durationInSecs>90</durationInSecs>
            <registrationTimestamp>1629099675720</registrationTimestamp>
            <lastRenewalTimestamp>1629963809206</lastRenewalTimestamp>
            <evictionTimestamp>0</evictionTimestamp>
            <serviceUpTimestamp>1629099675647</serviceUpTimestamp>
        </leaseInfo>
        <metadata>
            <management.port>8080</management.port>
        </metadata>
        <homePageUrl>http://hcm-core-base.staging-12.svc.k8s.staging.mokahr.com:8080/</homePageUrl>
        <statusPageUrl>http://hcm-core-base.staging-12.svc.k8s.staging.mokahr.com:8080/actuator/info</statusPageUrl>
        <healthCheckUrl>http://hcm-core-base.staging-12.svc.k8s.staging.mokahr.com:8080/actuator/health</healthCheckUrl>
        <vipAddress>hcm-core-base</vipAddress>
        <secureVipAddress>hcm-core-base</secureVipAddress>
        <isCoordinatingDiscoveryServer>false</isCoordinatingDiscoveryServer>
        <lastUpdatedTimestamp>1629099675720</lastUpdatedTimestamp>
        <lastDirtyTimestamp>1629099675637</lastDirtyTimestamp>
        <actionType>ADDED</actionType>
    </instance>
    <instance>
        <instanceId>10.88.49.199:hcm-core-base:8038</instanceId>
        <hostName>10.88.49.199</hostName>
        <app>HCM-CORE-BASE</app>
        <ipAddr>10.88.49.199</ipAddr>
        <status>UP</status>
        <overriddenstatus>UNKNOWN</overriddenstatus>
        <port enabled="true">8038</port>
        <securePort enabled="false">443</securePort>
        <countryId>1</countryId>
        <dataCenterInfo class="com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo">
            <name>MyOwn</name>
        </dataCenterInfo>
        <leaseInfo>
            <renewalIntervalInSecs>10</renewalIntervalInSecs>
            <durationInSecs>90</durationInSecs>
            <registrationTimestamp>1629962631974</registrationTimestamp>
            <lastRenewalTimestamp>1629963810284</lastRenewalTimestamp>
            <evictionTimestamp>0</evictionTimestamp>
            <serviceUpTimestamp>1629962631713</serviceUpTimestamp>
        </leaseInfo>
        <metadata>
            <grayReleased>1</grayReleased>
            <management.port>8038</management.port>
        </metadata>
        <homePageUrl>http://10.88.49.199:8038/</homePageUrl>
        <statusPageUrl>http://10.88.49.199:8038/actuator/info</statusPageUrl>
        <healthCheckUrl>http://10.88.49.199:8038/actuator/health</healthCheckUrl>
        <vipAddress>hcm-core-base</vipAddress>
        <secureVipAddress>hcm-core-base</secureVipAddress>
        <isCoordinatingDiscoveryServer>false</isCoordinatingDiscoveryServer>
        <lastUpdatedTimestamp>1629962631974</lastUpdatedTimestamp>
        <lastDirtyTimestamp>1629962631702</lastDirtyTimestamp>
        <actionType>ADDED</actionType>
    </instance>
</application>
```





**服务准备：**

![image2021-8-26_15-39-49](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281721012.png)

备注：10.88.49.199为灰度服务





**验证结果：**

```http
curl -X GET \
  http://localhost:8004/api/mobile/v1/get/config \
  -H 'Content-Type: application/json' \
  -H 'Postman-Token: 84a3d0b4-3742-42d4-ad8c-9e32da7c871c' \
  -H 'cache-control: no-cache' \
  -H 'hcm-user: {"uid":1,"tid":1}' \
  -d '{}'
```

`HCM-GATEWAY`日志如下：

![image2021-8-26_15-57-59](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281723387.png)



最后总结一下：





<img src="https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281726651.png" alt="image2021-9-24_16-47-59" style="zoom:50%;" />













### 3.1 发布流程

**总览：**

![image-20231128161329172](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281613233.png) **步骤I:**

删除旧pre分支：（必须通过gitlab操作，这里以某个前端项目为例）

![image2022-3-22_1-12-39](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281620550.png)



从master拉出新pre分支：

![image2022-3-22_1-13-46](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281620715.png)



部署pre环境，通过jenkins操作：

![image-20231128161734272](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281617324.png)



**步骤II**

创建MR将pre分支合并入gray-release分支：

![image2022-3-24_1-0-34](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281621163.png)







部署到灰度环境：

http://k8s-jenkins.mokahr.com/view/Gray-HCM/

![image2022-3-22_1-18-58](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281621035.png)





**步骤III**

创建MR(`Merge Request`)将gray-release分支合并入release分支：

![image2022-3-22_1-20-38](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281623465.png)





### 3.2 hotfix流程

**总览：**

通用的规则是：如果release分支变化了，立即合并回gray-release分支和master分支；如果gray-release分支变化了，立即合并回master分支。release上的hotfix是不会经过灰度的，必须慎重

![image-20231128162503338](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281625390.png)

有了灰度之后，调查bug的时候需要先分辨bug是属于灰度还是属于正式版，因为这将影响你再哪个分支上fix。推荐的分辨方法见[FAQ](#grayReleaseFAQ)



**详细步骤：**

在release分支上hotfix：

将release分支合并回gray-release分支（发布生产环境后，jenkins自动执行这个动作），同时gray-release也要做一次发布。

将release分支合并回master分支（发布生产环境后，jenkins自动执行这个动作）



在gray-release分支上hotfix：

将gray-release分支合并回master分支（发布gray-release环境后，jenkins自动执行这个动作）





### 3.3 回滚流程

(1) 如果是gray-release环境需要回滚

直接清空灰度列表。

gray-release回滚之后可以不revert，修完了再把流量切回来即可。

注：如果存在刷数据等不兼容的情况，无法回滚（这个跟是否灰度无关）



(2) 如果是release环境需要回滚

按照正常回滚流程

这里有个比较尴尬的情况：如果release环境需要回滚代码，通常代表这是一个比较严重的历史问题，那么理论上gray-release也要回滚（因为gray-release带有release的改动）。

所以release回滚会连带着gray-release的回滚，具体操作方法见上面的”如果是gray-release环境需要回滚“。

release回滚之后需要revert代码，待bug被彻底修复后，再发布上线，这个过程就跟hotfix流程一致了，具体操作方法见上文的”hotfix流程“。





### 3.4 灰度放量



**总览：**

![image-20231128162955220](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281629285.png)



**灰度一阶段（内部验证）**

周四晚灰度发布后

灰度名单为测试租户

在研发大群里通知所有人灰度名单，各模块owner关注监控和报警



**灰度二阶段**

灰度发布后的周一上午

灰度名单5家租户：Moka+灰度名单

在csm和研发大群里通知所有人灰度名单，各模块owner关注监控和报警



**灰度三阶段**

灰度发布后的周三上午

灰度名单15家租户：Moka+灰度名单

在csm和研发大群里通知所有人灰度名单，各模块owner关注监控和报警







###  <a id="grayReleaseFAQ">灰度发布FAQ</a>





#### 1、为什么会有无法灰度的项目？



目前的灰度分流原理是网关层对moka-token解析得到租户id后分流，因此如果一个项目跟租户完全无关（例如硬件设备对接等基础服务），或者使用非moka-token的方式做登录校验（例如notoken系列接口），则这种项目是无法被灰度的，此外还有一些特殊情况会导致项目无法灰度。

大致有以下几种情况：

1. 前端发起的notoken系列接口。比如信息采集一系列的/notoken项目，notoken不经过passport，登录方式与普通api调用不同，暂时无法实现灰度分流，未来会改造支持。
2. 前端改造了html模板内容，或者改动了nodejs的东西。html请求无法携带自定义header token，因此流量无法染色，不支持灰度，未来改造会支持（染色换成cookie）。
3. CSM后台（宙斯）项目。目前暂未完成灰度改造，未来会改造支持。
4. 后端openapi业务。登录校验方式与普通api调用不同，暂时无法实现灰度，未来会改造支持。
5. 后端跟租户无关的基础服务。比如第三方回调等，请求链路中没有租户信息。
6. 后端个别定时任务。定时任务需要经过一定改造后才可以支持灰度
7. 依赖了一些无法灰度的第三方服务，比如依赖了BI，但BI暂时无法灰度。



:question: 无法灰度的项目如何发布 :question:

无法灰度的项目不能参与灰度发布（废话），需要单独上线（直接合并入release后上线），类似对release分支做hotfix。

特别注意，如果一个任务同时涉及灰度和非灰度的项目改动，默认这个任务所有的改动都不能走灰度发布（否则可能会串），当然如果改动是兼容的串了也没关系是例外。

目前没有特别好的强制管理办法，依赖各模块TL把控，每次迭代上线前需要确认符合本规范。



**无法灰度的项目列表如下：**

注：持续收集中

| 前后端 | 项目名称                  | 描述               | owner | 无法灰度原因                                                 | 未来是否可以改造支持灰度 |
| :----- | :------------------------ | :----------------- | :---- | :----------------------------------------------------------- | :----------------------- |
| 前端   | `captain`                 | `信息采集前端项目` |       | 使用notoken标识用户信息，跟常规moka-token不同目前暂不支持灰度分流 | 未来可改造支持           |
| 前端   | zeus                      | csm后台项目        | 富国  | 前端暂未完成灰度改造                                         | 未来可改造支持           |
| 后端   | websocket                 |                    |       | 硬件设备链接时，没有租户信息支持灰度                         | 无法改造                 |
| 后端   | moka-infrastructure-esign | 电子签业务         |       | 部分模块不可灰度1、应用授权成功e签宝回调2、签署流程结果回调接口3、印章变动回调原因：回调时没有租户信息。 | 无法改造                 |
| 后端   | hcm-open-api              |                    |       | 为客户提供的接口api接口                                      |                          |
| 后端   | passport                  | 平台架构passport   |       | passport→ hcm-mobile 对接，passport转发打卡信息无法获取租户信息 |                          |





#### 灰度发布以后，开发分支从哪里checkout出来？

跟以前一样，但稍微有些不同。

原来大家习惯从release分支拉开发分支，是因为很多任务是单独迭代，如果从master拉分支，上线的时候可能不小心把其他人的东西误带上线。

但现在我们采取灰度发布后，废弃了单独迭代的概念，因此大家可以放心从master上拉分支了，因为是统一进集成，统一灰度，统一上线，就不用担心误把其他人的东西带上线了。

当然，如果你还是习惯从release拉分支，也不是不可以，比较无脑。但有一种情况是一个需求被拆成两半，一半先灰度上线，紧接着要开发剩下一半，此时如果从release拉分支是没有前一半的代码改动的（因为还在gray-release上）。对于这种特殊情况，可以从master直接拉开发分支。

注意：无法灰度的项目，还是需要走单独上线



#### 3、如何判断一个bug是灰度的问题还是正式的问题？

光看报错的环境是不够的，因为灰度环境的错误，可能在正式环境也有，只不过是在灰度环境报出来了，所以需要再调查：

如果在正式环境上复现了，那这是个历史遗留问题，需要在release分支修复，而且大概率灰度也有问题。

如果正式环境没有复现，灰度环境上复现了，那这大概率就是一个新问题，需要去gray-release分支修复。

当然，可以看一下本次灰度都上了什么功能，是否跟这个bug有关，也可以有一些辅助判断。





#### 4、如果灰度验证阶段发现大的问题，导致无法按时全量上线，对下一次迭代造成影响怎么办？

如果非常不幸出现这个问题，需要单独讨论沟通（综合客户侧、产品侧、研发侧）给出决策，比如推迟全量时间等等。





#### 如果没有单独迭代了，某个业务模块出问题导致整个版本无法发布怎么办？

是可能出现这种情况的，这也是集体发版必然会导致的结果，只能说尽量避免吧，如果真的出现了就需要周知相关各方。

其实我们的终极理想是做到各模块独立发布上线，互不影响，但目前技术底层架构还暂时我发做到这一点，需要逐渐改造和过度。

一体化需求两边的灰度节奏需要保持一致

- 如果一边在灰度，另一边不在灰度，那肯定测试阶段能发现，也就会解决这个问题。

- 如果两边都在灰度，灰度环境测试不出来，结果一边发了全量，另一边还没全量，这个时候就会出问题。

- 需要在上线计划中周知到，两边保持一致，好像除了靠人之外，暂时没有什么有效解决办法。





## 5、Code Review



### 代码review规范

**初级目的：**

1、代码结构是否符合方案设计预期。

2、代码细节是否符合代码规范，参考代码规范

**深层次目的：**

1、提高代码的可读性和可维护性

2、降低团队协作的成本



### 代码review流程

1. approve人至少包含2个人以上，核心服务，如core-base等可以考虑3人及以上approve才能merge。
2. 只有TL有merge权限。（收缩权限，降低风险，以及TL对本组业务的把控）
3. 回收master的push权限，master的代码变更只能通过merge request进行。
4. 代码review人与代码提交人，在出现线上故障时，共同承担责任。





**代码merge request 提出人的责任：**

1. 在规定时间之前提出merge request 并且周知到代码review人
2. 在代码发pre环境之前，push review人review完代码
3. pre环境验收完成之后，通知TL merge代码
4. 代码review人必须是一起去参加需求评审，对业务有一定了解。
5. review人包含TL。

**代码 review人的责任：**

1. 在代码上pre环境之前review完代码，单独迭代需求还包括在提测之前review完代码结构是否负责方案设计。
2. 严格按照代码规范review代码，对于代码规范中强制要求的部分，不满足的打回。

**TeamLeader 的职责：**

1. 负责在上线前merge代码
2. 负责在上线前确认代码approve之后有多少代码改动，并确认改动是否已经周知到QA测试。**如果改动较大，或者QA没有验证，或者QA评估验证成本较高，延后上线。**



### 代码review需要重点关注点建议汇总

1. 重复代码抽取
2. 魔数
3. 是否有可能空指针
4. 是否有循环DB调用
5. 是否有sql拼接
6. 是否通篇没注解
7. 异常处理是否有日志，日志内容是否有效
8. 注解是否全面且有效









# Kubernetes





## Moka架构图



1、Moka K8s 架构图：



<img src="https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281822899.png" alt="image2022-10-21_17-43-21"  />





首先看一下架构图，目前规划方案总共有三个集群：

Ops运维基础集群、Staging测试集群、Proc生产环境集群

![image2020-12-24_16-28-45](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281831641.png)



然后来看一下实际集群分布情况：

![image-20231201143202531](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011432788.png)



开发自测都是在`Staging`测试环境中，可以看到有多个泳道环境：

![image-20231201143404109](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011434361.png)



通过CI/CD流水线在内部测试平台部署镜像到Staging-36环境后，即可在KuBoard观察到集群的健康状况：

![image-20231201144109509](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011441770.png)



Kuboard界面内:

![image-20231201144216817](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011442071.png)



查看 hcm-csm 服务，可看到该服务当前镜像的状态：

![image-20231201144728222](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202312011447489.png)













2、Moka eureka架构图：

![image2022-10-21_17-46-9](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281821789.png)





3、Moka 网关架构图：

![image2022-10-21_17-47-25](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281823869.png)









4、Moka 网络架构图

![image2022-10-21_17-40-37](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281824607.png)









## CI/CD流程

基于 Jenkins 的 CI/CD 流程如下所示：

![img](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281752528.png)



应用构建和发布流程说明如下：

1. 用户向 Gitlab 提交代码，代码中必须包含 `Dockerfile`
2. 将代码提交到远程仓库
3. 用户在发布应用时需要填写 git 仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发 Jenkins 自动构建
4. Jenkins 的 CI 流水线自动编译代码并打包成 docker 镜像推送到 Harbor 镜像仓库
5. Jenkins 的 CI 流水线中包括了自定义脚本，根据我们已准备好的 kubernetes 的 YAML 模板，将其中的变量替换成用户输入的选项
6. 生成应用的 Kubernetes YAML 配置文件
7. 更新 Ingress 的配置，根据新部署的应用的名称，在 ingress 的配置文件中增加一条路由信息









目前Moka内部的CI/CD流程图如下：

![image2021-8-4_15-53-30](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281757362.png) 

**主要接入流程如下：**

1.在项目git目录中添加.gitlab-ci.yaml文件，以及ops文件夹

2.在pipeline项目中的apps文件夹添加应用的valus目录

3.在Jenkins中添加对应的任务



开发人员需要在自己项目中添加文件： 

执行以下脚本自动生成gitlab-ci所需文件

wget http://home.mokahr.com/ops-tools/init_git_ops.sh && /bin/bash init_git_ops.sh

根据内容填写完会有三个文件放在项目下面

.gitlab-ci.yml 、 ops/Dockerfile 、  **ops/entrypoint.sh**



三个文件需要注意事项如下：

.gitlaba-ci.yaml 需要修改部门，以及开发语言：

```yaml
Build Image:
  stage: build
  image: registry.mokahr.com/ops/base:docker
  script:
    #初始化运维环境
    - sh /entrypoint.sh
    #查看sonar检查是否通过，不通过不可以生成镜像
    - sh /sonarcheck.sh ${CI_COMMIT_BRANCH} ${CI_PROJECT_NAME}
    ##构建镜像
    - docker build -t ${ALI_IMAGE_URL} -t ${TC_IMAGE_URL} -f ops/Dockerfile .
    ##推送镜像
    - docker push ${ALI_IMAGE_URL}
    - echo -e "本次构建镜像版本:\n${IMAGE_VERSION}\n本次构建镜像地址\n阿里云:${ALI_IMAGE_URL}\n腾讯云:${TC_IMAGE_URL}"
    - sh /ci-end.sh build_image ${CI_COMMIT_AUTHOR} ${DEP} ${CI_PROJECT_NAME} ${IMAGE_VERSION} ${CI_COMMIT_TAG}
```



Dockerfile根据不同语言，配置不同的构建目录

entrypoint.sh根据不同环境配置不同的启动命令



### 新版CI/CD发布流程-开发操作



**首先明确一个概念：镜像版本**

date-commit-branch/tag  例子：20210820-1454-655d4-master

镜像版本格式说明：前两位为日期和时间，中间为commit信息前五位，最后为branch信息或者tag信息，tag信息优先级大于branch



**首次使用-使用前检查文件：**

确保当前分支代码中有 .gitlab-ci.yaml 以及 ops 文件夹

默认运维会初始化完成这写文件

新应用接入：请联系运维创建

**一、手动发布 (支持环境：prod，staging)**

默认开放所有分支自动生成镜像

想要做限制的可以自行修改项目的.gitlab-ci.yml文件

在build的步骤下面加上自定义规则，下面是一些示例写法

only:

 \- /^staging.*/

 \- tags

 \- master

 \- release



确保镜像生成：

提交代码后等待，无需操作，默认会自动构建镜像，会有通知发送给本次commit提交人

构建失败怎么办：

- 通过gitlab的界面进入自己的项目后，进入CI/CD，然后查看pipeline，找到历史的构建任务，查看Build这一步里面的镜像版本

！！！！！！以上步骤无需操作，自动生成！！！！！！



去jenkins上发布：

选择环境namespace

选择应用(可能会有连带应用)。

镜像选择：可以直接动态搜索镜像关键词。默认以时间倒序排列，首选镜像一定是最新的镜像。

直接build即可

![](https://cdn.jsdelivr.net/gh/amonstercat/PicGo/202311281928442.png)









### DockerFile

以 **hcm-auth** 为例：

```dockerfile
#java Dockerfile

#构建jar包
FROM  registry.mokahr.com/ops/base:maven as builder

##根据git项目名称填写
ARG APP_NAME=hcm-auth
ARG BUILD_JAR=target/hcm-auth-0.0.5-RELEASE.jar

ENV PROJECT_ROOT=/project
WORKDIR $PROJECT_ROOT

COPY . $PROJECT_ROOT

RUN  mvn clean package -Dmaven.test.skip=true -U \
	 &&  mkdir -pv output/ \
	 &&  cp -v ${BUILD_JAR} output/${APP_NAME}.jar



#构建运行镜像
FROM  registry.mokahr.com/ops/base:java

ENV COMMIT_SHA=CI_COMMIT_ID
ENV COMMIT_BRANCH=CI_BRANCH
ENV COMMIT_TAG=CI_TAG
ENV PROJECT_ROOT=/project
WORKDIR $PROJECT_ROOT

RUN java -version

COPY  --from=builder $PROJECT_ROOT/output  $PROJECT_ROOT

COPY ops/entrypoint.sh /entrypoint.sh

ENTRYPOINT  ["/bin/bash","-x","/entrypoint.sh"]

```

主要分为两个阶段：



- [ ]  **第一阶段 (构建jar包):**
  - 使用基于Maven的基础镜像，作为构建阶段的基础环境。
  - 通过`ARG`指令设置构建过程中的一些参数，例如`APP_NAME`和`BUILD_JAR`。
  - 设置环境变量`PROJECT_ROOT`并在该目录下工作。
  - 将当前目录下的所有内容复制到`$PROJECT_ROOT`。
  - 使用Maven构建项目，跳过测试，然后将构建的jar文件复制到输出目录。



- [ ]  **第二阶段 (构建运行镜像):**
  - 使用基于Java的基础镜像，作为运行阶段的基础环境。
  - 设置一些环境变量，如`COMMIT_SHA`、`COMMIT_BRANCH`和`COMMIT_TAG`，这些变量似乎是从CI/CD工具中获取的。
  - 在工作目录下打印Java版本。
  - 从第一阶段（构建jar包的阶段）复制`$PROJECT_ROOT/output`目录的内容，包括构建好的jar文件。
  - 复制`ops/entrypoint.sh`脚本到容器中。
  - 设置`ENTRYPOINT`，以便容器启动时执行`/entrypoint.sh`脚本。









# Bug排查









## 钉钉三方打卡记录未同步











## 自定义公式上下文处理











## CSM远程调用接口失败原因







